---
title: "Peak Bloom Prediction"
author: "James Miller"
date: "02/28/2025"
lang: en-US
format:
  html:
    embed-resources: true
---

```{r}
#| label: setup
#| include: false
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, error = FALSE, 
                      message = FALSE,
                      fig.align = 'center',
                      out.width = '80%')

library(tidyverse)
library(lubridate)
library(rvest)
library(dplyr)
library(ggplot2)
library(factoextra)
library(randomForest)
```

Starting off I want to bring in data I downloaded from NOAA (When it was actually cooperating). This data was obtained from the historical recordings from the Reagan Airport station. These observations are the minimum and maximum temperatures in fahrenheit that were then converted to celsius and to avoid NA values the average temperature was gotten by adding and dividing the two values by two. Then the growing degree days were calculated and the cumulative sum was taken for the accumulated heating units. Months from January to May were only selected to trim the data set down a bit and eventually only 120 days into the year were kept due to the latest historical recording available is day 108.

After the 120 days were grabbed, the dataset was pivoted such that each row corresponds to a year while each column corresponds to a day of the year. This resulted in 121 columns (bloom day being the extra).

## Personal Data

```{r}
dc <- read.csv("data\\3924792.csv") %>%
  mutate(
    DATE = as.Date(DATE, format = "%m/%d/%Y"),
    YEAR = year(DATE),
    MONTH = month(DATE),
    TMAX = ((TMAX-32) * (5/9)),
    TMIN = ((TMIN-32) * (5/9)),
    TAVG = (TMAX+TMIN) / 2,
    GDD = ifelse(TAVG > 0, TAVG^2, 0)
  ) %>%
  group_by(YEAR) %>%
  mutate(
    DOY = row_number()
  ) %>%
  ungroup(YEAR) %>%
  filter(
    MONTH %in% c(1:5)
  )

dc_bloom <- read.csv("data\\washingtondc.csv") %>%
  filter(
    year %in% c(1942:2024)
  ) %>%
  mutate(
    DATE = as.Date(bloom_date, format = "%Y-%m-%d")
  ) %>%
  select(c(DATE, bloom_doy))

bloom_data <- left_join(dc, dc_bloom, by = join_by(DATE)) %>%
  mutate(
    BLOOM = ifelse(is.na(bloom_doy), 0, 1),
  ) %>%
  select(
    -c(bloom_doy, TAVG, TMAX, TMIN, MONTH)
  ) %>%
  group_by(YEAR) %>%
  mutate(
    GROWING_UNITS = cumsum(GDD)
  ) %>%
  filter(
    DOY %in% c(1:120)
  ) %>%
  ungroup(YEAR)

wide <- bloom_data %>%
  select(YEAR, DOY, GROWING_UNITS) %>%
  pivot_wider(
  names_from = DOY,
  values_from = GROWING_UNITS,
  names_prefix = "DOY_"
) %>%
  filter(
    YEAR!=2025
  ) %>%
  mutate(
    BLOOM_DOY = dc_bloom$bloom_doy
  ) %>%
  select(
    -YEAR
  )
```  
  
The data above is going to be used to train the model, working on the assumption that trees in DC are functionally the same as the trees in the other blooming locations. 

Now the below code was granted from Professor Auerbach (thank you so much!) and will be used to capture the most up to date data in order to predict the bloom dates. This code was honestly a massive help given the current goings on within NOAA and their data catalog.

## Data Queries

```{r}
get_weather_table <- function(url)
  read_html(url) %>% 
  html_nodes("div.monthly-calendar") %>% 
  html_text2() %>%
  str_replace("N/A", "N/A N/A") %>%
  str_remove_all("Â°|Hist. Avg. ") %>%
  str_split(" ", simplify = TRUE) %>%
  parse_number() %>%
  matrix(ncol = 3, 
         byrow = TRUE,
         dimnames = list(NULL, c("day", "tmax", "tmin"))) %>%
  as_tibble() %>%
  filter(
    row_number() %in%
      (which(diff(day) < 0) %>% (function(x) if(length(x) == 1) seq(1, x[1], 1) else seq(x[1] + 1, x[2], 1))))

kyoto <-
  tibble(
    base_url = "https://web.archive.org/web/20250225/https://www.accuweather.com/en/jp/arashiyama/2334469/",
    month = month.name[1:4],
    year = 2025,
    url = str_c(base_url, tolower(month), "-weather/2334469?year=", year)) %>%
  mutate(temp = map(url, get_weather_table)) %>%
  pull(temp) %>%
  reduce(bind_rows) %>%
  transmute(date = seq(as.Date("2025-01-01"), as.Date("2025-04-30"), 1),
            year = parse_number(format(date, "%Y")),
            tmax,
            tmin,
            temp = (tmax + tmin) / 2)

#liestal march
liestal <-
  tibble(
    base_url = "https://web.archive.org/web/20250225/https://www.accuweather.com/en/ch/liestal/311994/",
    month = month.name[1:4],
    year = 2025,
    url = str_c(base_url, tolower(month), "-weather/311994?year=", year)) %>%
  mutate(temp = map(url, get_weather_table)) %>%
  pull(temp) %>%
  reduce(bind_rows) %>%
  transmute(date = seq(as.Date("2025-01-01"), as.Date("2025-04-30"), 1),
            year = parse_number(format(date, "%Y")),
            tmax,
            tmin,
            temp = (tmax + tmin) / 2)  

newyork <-
  tibble(
    base_url = "https://web.archive.org/web/20250225/https://www.accuweather.com/en/us/new-york/10021/",
    month = month.name[1:4],
    year = 2025,
    url = str_c(base_url, tolower(month), "-weather/349727?year=", year)) %>%
  mutate(temp = map(url, get_weather_table)) %>%
  pull(temp) %>%
  reduce(bind_rows) %>%
  transmute(date = seq(as.Date("2025-01-01"), as.Date("2025-04-30"), 1),
            year = parse_number(format(date, "%Y")),
            tmax,
            tmin,
            temp = (tmax + tmin) / 2)

washington <-
  tibble(
    base_url = "https://web.archive.org/web/20250225/https://www.accuweather.com/en/us/washington/20006/",
    month = month.name[1:4],
    year = 2025,
    url = str_c(base_url, tolower(month), "-weather/18-327659_1_al?year=", year)) %>%
  mutate(temp = map(url, get_weather_table)) %>%
  pull(temp) %>%
  reduce(bind_rows) %>%
  transmute(date = seq(as.Date("2025-01-01"), as.Date("2025-04-30"), 1),
            year = parse_number(format(date, "%Y")),
            tmax,
            tmin,
            temp = (tmax + tmin) / 2)  
    
vancouver <-
  tibble(
    base_url = "https://web.archive.org/web/20250225/https://www.accuweather.com/en/us/vancouver/98661/",
    month = month.name[1:4],
    year = 2025,
    url = str_c(base_url, tolower(month), "-weather/331419?year=", year)) %>%
  mutate(temp = map(url, get_weather_table)) %>%
  pull(temp) %>%
  reduce(bind_rows) %>%
  transmute(date = seq(as.Date("2025-01-01"), as.Date("2025-04-30"), 1),
            year = parse_number(format(date, "%Y")),
            tmax,
            tmin,
            temp = (tmax + tmin) / 2)  
```  
  
## Modelling

Now getting into the actual modelling. My initial idea is naturally to use Random Forest, but along with Random Forest I will be attempting other types of models, with the limit being we have shrunk our data set in terms of observations but have a large number of columns. Dimension reduction will be performed to further reduce the columns to hopefully arrive at a model that performs best above the others.

### Dimension Reduction

In order to control the number of columns we will have, I will be performing Principal Component Analysis to bring our 121 features down into a more manageable level.

```{r}
wide_train <- wide[1:68,]
wide_train_x <- wide_train %>%
  select(-BLOOM_DOY)
wide_train_y <- wide_train$BLOOM_DOY

wide_test <- wide[69:83,]
wide_test_x <- wide_test %>%
  select(-BLOOM_DOY)

wide_test_y <- wide_test$BLOOM_DOY

scale_train_x <- as.data.frame(scale(wide_train_x))

pca <- prcomp(scale_train_x, center = T, scale. = T)

scale_test_x <- as.data.frame(scale(wide_test_x, center = pca$center, scale = pca$scale))

wide_test_x_proj <- as.data.frame(predict(pca, newdata = wide_test_x)) %>%
  select(
    PC1:PC3
  )

wide_train <- wide_train_x
wide_train$y <- wide_train_y

pca_train <- as.data.frame(pca$x[,1:3])
pca_train$y <- wide_train_y

fviz_eig(pca)
```

### Model Training  
  
```{r}
set.seed(13)

rf_mtry <- tuneRF(wide_train_x, wide_train_y, stepFactor = 1.5, improve = 0.01, ntreeTry = 10000)
rf_pca_mtry <- tuneRF(pca_train[,-4], pca_train$y, stepFactor = 1.5, improve = 0.01, ntreeTry = 10000)

rf <- randomForest(x = wide_train_x, y = wide_train_y, mtry = 13, ntree = 10000)
rf_pca <- randomForest(x = pca_train[,-4], y = pca_train$y, mtry = 1, ntree = 10000)

rf_pred <- predict(rf, newdata = wide_test_x)
rf_pca_pred <- predict(rf_pca, newdata = wide_test_x_proj)

mae_pred <- sum(abs((round(rf_pred, 0) - wide_test_y)))
mae_pred_pca <- sum(abs((round(rf_pca_pred, 0) - wide_test_y)))

prep_data <- function(data, blooms){
  
  copy <- data
  
  bloom <- read.csv(blooms) %>%
    mutate(
      YEAR = year(bloom_date)
    ) %>%
    filter(
      YEAR %in% c(1942:2024)
    ) %>%
    mutate(
      DATE = as.Date(bloom_date, format = "%Y-%m-%d")
    ) %>%
    select(c(DATE, bloom_doy))
  
  bloom_join <- left_join(copy, bloom, by = join_by(DATE)) %>%
    mutate(
      BLOOM = ifelse(is.na(bloom_doy), 0, 1),
    ) %>%
    select(
      -c(bloom_doy, TAVG, TMAX, TMIN)
    ) %>%
    group_by(YEAR) %>%
    mutate(
      GROWING_UNITS = cumsum(GDD)
    ) %>%
    filter(
      DOY %in% c(1:120)
    ) %>%
    ungroup(YEAR) %>%
    filter(
      YEAR!=2025
    )
  
  bloom_wide <- bloom_join %>%
    select(YEAR, DOY, GROWING_UNITS) %>%
    pivot_wider(
    names_from = DOY,
    values_from = GROWING_UNITS,
    names_prefix = "DOY_"
    ) %>%
    select(
      -YEAR
    )
  
  bloom_doy <- left_join(bloom_join, bloom, by = join_by(DATE)) %>%
    filter(!is.na(bloom_doy)) %>%
    select(bloom_doy, YEAR)
  
  bloom_wide$bloom_doy <- bloom_doy$bloom_doy
  
  return(bloom_wide)
}

train_model <- function(wide){
  set.seed(13)
  
  wide_train <- wide[1:68,]
  wide_train_x <- wide_train %>%
    select(-bloom_doy)
  wide_train_y <- wide_train$bloom_doy

  wide_test <- wide[69:83,]
  wide_test_x <- wide_test %>%
    select(-bloom_doy)

  wide_test_y <- wide_test$bloom_doy
  
  rf_mtry <- tuneRF(wide_train_x, wide_train_y, stepFactor = 1.5, improve = 0.01, ntreeTry = 10000)
  
  rf <- randomForest(x = wide_train_x, y = wide_train_y, mtry = rf_mtry[which(rf_mtry[,2] == min(rf_mtry[,2]))], ntree = 10000)
  
  return(rf)
}

prep_test <- function(data){
  copy <- data %>%
  mutate(
    DATE = as.Date(date, format = "%m/%d/%Y"),
    YEAR = year(DATE),
    MONTH = month(DATE),
    TMAX = ((tmax-32) * (5/9)),
    TMIN = ((tmin-32) * (5/9)),
    TAVG = (tmax+tmin) / 2,
    GDD = ifelse(TAVG > 0, TAVG^2, 0),
    GROWING_UNITS = cumsum(GDD),
    DOY = row_number() 
  ) %>%
    select(-c(tmax, tmin, date, year)) %>%
    select(YEAR, DOY, GROWING_UNITS) %>%
    pivot_wider(
    names_from = DOY,
    values_from = GROWING_UNITS,
    names_prefix = "DOY_"
    ) %>%
    select(
      -YEAR
    )
}
```

### Predictions

```{r}
# Training Data for each individual model
washington_prep <- prep_data(dc, "data\\washingtondc.csv")

kyoto_sub <- dc %>%
  filter(YEAR != 1945)

liestal_sub <- dc %>%
  filter(YEAR != 1986)

kyoto_prep <- prep_data(kyoto_sub, "data\\kyoto.csv")
liestal_prep <- prep_data(liestal_sub, "data\\liestal.csv")
newyork_prep <- prep_data(dc, "data\\washingtondc.csv")
vancouver_prep <- prep_data(dc, "data\\washingtondc.csv")

# Individual Models Training
washington_model <- train_model(washington_prep)
kyoto_model <- train_model(kyoto_prep)
liestal_model <- train_model(liestal_prep)
newyork_model <- train_model(newyork_prep)
vancouver_model <- train_model(vancouver_prep)

# Prepping the Testing Data
washington_test <- prep_test(washington)
kyoto_test <- prep_test(kyoto)
liestal_test <- prep_test(liestal)
newyork_test <- prep_test(newyork)
vancouver_test <- prep_test(vancouver)
                        
# Predictions
washington_pred <- round(predict(washington_model, newdata = washington_test), 0)
kyoto_pred <- round(predict(kyoto_model, newdata = kyoto_test), 0)
liestal_pred <- round(predict(liestal_model, newdata = liestal_test), 0)
newyork_pred <- round(predict(newyork_model, newdata = newyork_test), 0)
vancouver_pred <- round(predict(vancouver_model, newdata = vancouver_test), 0)

# Prediction Intervals
```




